{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k7dAiocKpAk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIzRhSoFKz4k",
        "outputId": "8c6adad3-b8e1-4746-d406-7746cb86eff6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Allowing to access google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht5zsftkVDhM"
      },
      "source": [
        "#When running this code, please adjust the file path to match the location of your file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPDRamW0U0nu",
        "outputId": "b2d328a8-7637-4b82-9faa-a769e5414a57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Studi_unbalanced\\Elchin_Latifli\n"
          ]
        }
      ],
      "source": [
        "#Getting the path\n",
        "path=os.getcwd()\n",
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-_D30NBK128"
      },
      "outputs": [],
      "source": [
        "# Defining the file path\n",
        "file_path = 'preprocessed_ethereum_data.csv'\n",
        "\n",
        "# Reading the CSV file\n",
        "data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN7dLe4RC0dk",
        "outputId": "e120fdae-343e-4ecc-a157-aad4b2e77bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9841 entries, 0 to 9840\n",
            "Data columns (total 40 columns):\n",
            " #   Column                                                Non-Null Count  Dtype  \n",
            "---  ------                                                --------------  -----  \n",
            " 0   isFraud                                               9841 non-null   int64  \n",
            " 1   Avg min between sent tnx                              9841 non-null   float64\n",
            " 2   Avg min between received tnx                          9841 non-null   float64\n",
            " 3   Time Diff between first and last (Mins)               9841 non-null   float64\n",
            " 4   Sent tnx                                              9841 non-null   float64\n",
            " 5   Received Tnx                                          9841 non-null   float64\n",
            " 6   Number of Created Contracts                           9841 non-null   float64\n",
            " 7   Unique Received From Addresses                        9841 non-null   float64\n",
            " 8   Unique Sent To Addresses                              9841 non-null   float64\n",
            " 9   min value received                                    9841 non-null   float64\n",
            " 10  max value received                                    9841 non-null   float64\n",
            " 11  avg val received                                      9841 non-null   float64\n",
            " 12  min val sent                                          9841 non-null   float64\n",
            " 13  max val sent                                          9841 non-null   float64\n",
            " 14  avg val sent                                          9841 non-null   float64\n",
            " 15  min value sent to contract                            9841 non-null   float64\n",
            " 16  max val sent to contract                              9841 non-null   float64\n",
            " 17  avg value sent to contract                            9841 non-null   float64\n",
            " 18  total transactions (including tnx to create contract  9841 non-null   float64\n",
            " 19  total Ether sent                                      9841 non-null   float64\n",
            " 20  total ether received                                  9841 non-null   float64\n",
            " 21  total ether sent contracts                            9841 non-null   float64\n",
            " 22  total ether balance                                   9841 non-null   float64\n",
            " 23  Total ERC20 tnxs                                      9841 non-null   float64\n",
            " 24  ERC20 total Ether received                            9841 non-null   float64\n",
            " 25  ERC20 total ether sent                                9841 non-null   float64\n",
            " 26  ERC20 total Ether sent contract                       9841 non-null   float64\n",
            " 27  ERC20 uniq sent addr                                  9841 non-null   float64\n",
            " 28  ERC20 uniq rec addr                                   9841 non-null   float64\n",
            " 29  ERC20 uniq sent addr.1                                9841 non-null   float64\n",
            " 30  ERC20 uniq rec contract addr                          9841 non-null   float64\n",
            " 31  ERC20 min val rec                                     9841 non-null   float64\n",
            " 32  ERC20 max val rec                                     9841 non-null   float64\n",
            " 33  ERC20 avg val rec                                     9841 non-null   float64\n",
            " 34  ERC20 min val sent                                    9841 non-null   float64\n",
            " 35  ERC20 max val sent                                    9841 non-null   float64\n",
            " 36  ERC20 avg val sent                                    9841 non-null   float64\n",
            " 37  ERC20 uniq sent token name                            9841 non-null   float64\n",
            " 38  ERC20 uniq rec token name                             9841 non-null   float64\n",
            " 39  involved_fraudulent_token                             9841 non-null   int64  \n",
            "dtypes: float64(38), int64(2)\n",
            "memory usage: 3.0 MB\n"
          ]
        }
      ],
      "source": [
        "#Checking the data info\n",
        "data.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gflM6C4fCdmb"
      },
      "source": [
        "**Without balancing + logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXvUfyaOK15h"
      },
      "outputs": [],
      "source": [
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for logistic regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'max_iter': [3000],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_none.csv'\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling, capturing the indices\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "\n",
        "    # Creating a Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # Creating a GridSearchCV object with the specified parameter grid\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Using the best model obtained from GridSearchCV\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, probabilities, run number, and original indices\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen \"C\" value for each run\n",
        "    print(f\"Run {i+1} completed. Chosen C: {best_params['C']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_none.csv.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQkYEpNnCkza"
      },
      "source": [
        "**Random undersampling + logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTJl1JBMK17z"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for logistic regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'max_iter': [3000],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_rus.csv'\n",
        "\n",
        "\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "\n",
        "    # Printing the number of majority and minority instances before undersampling\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before undersampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying random undersampling to the training data\n",
        "    rus = RandomUnderSampler(random_state=None)\n",
        "    X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after undersampling\n",
        "    print(\"After undersampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Creating a Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # Creating a GridSearchCV object\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen \"C\" value for each run\n",
        "    print(f\"Run {i+1} completed. Chosen C: {best_params['C']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_rus.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw_Sg-DcCvif"
      },
      "source": [
        "**Tomek link + logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlsE34lVK1_Y"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Define the parameter grid for logistic regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'max_iter': [3000],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_tomek.csv'\n",
        "\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying Tomek Links\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before Tomek Links:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying Tomek Links to the training data\n",
        "    tl = TomekLinks(sampling_strategy='majority')\n",
        "    X_train_res, y_train_res = tl.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying Tomek Links\n",
        "    print(\"After Tomek Links:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Creating a Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # Creating a GridSearchCV object\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    print(f\"Run {i+1} completed. Chosen C: {grid_search.best_params_['C']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_tomek.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y86CZi6EQkj"
      },
      "source": [
        "**Cluster ethereumd majority undersampling + logisitc regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HcjuEYbvPbp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Define the cluster-ethereumd undersampling method\n",
        "def cluster_ethereumd_undersampling(X_train, y_train, n_clusters):\n",
        "    # Separating majority and minority classes\n",
        "    X_train_maj = X_train[y_train == 0]\n",
        "    X_train_min = X_train[y_train == 1]\n",
        "\n",
        "    # Applying KMeans clustering to the majority class\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10)\n",
        "    kmeans.fit(X_train_maj)\n",
        "    cluster_labels = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    # Calculating the number of samples to keep from each cluster\n",
        "    N_maj = len(X_train_maj)\n",
        "    N_min = len(X_train_min)\n",
        "    N_maj_i = np.bincount(cluster_labels)\n",
        "    r_i = N_maj_i / N_maj\n",
        "    s_i = np.round(N_min * r_i).astype(int)\n",
        "\n",
        "    # Selecting the nearest samples to the centroids\n",
        "    X_resampled_maj = []\n",
        "    for i in range(kmeans.n_clusters):\n",
        "        cluster_points = X_train_maj[cluster_labels == i]\n",
        "        centroid = centroids[i].reshape(1, -1)\n",
        "        distances = np.linalg.norm(cluster_points - centroid, axis=1)\n",
        "        sorted_indices = np.argsort(distances)\n",
        "        X_resampled_maj.append(pd.DataFrame(cluster_points).iloc[sorted_indices[:s_i[i]]])\n",
        "\n",
        "    X_resampled_maj = pd.concat(X_resampled_maj)\n",
        "    X_resampled = pd.concat([X_resampled_maj, X_train_min])\n",
        "    y_resampled = np.array([0] * len(X_resampled_maj) + [1] * len(X_train_min))\n",
        "\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# Define the parameter grid for logistic regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'max_iter': [3000],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_cbmu.csv'\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Calculate the within-cluster sum of squares (WCSS) for different number of clusters\n",
        "    wcss = []\n",
        "    for j in range(2, 11):  # Start from 2 to avoid choosing 1 as the optimal number of clusters\n",
        "        kmeans = KMeans(n_clusters=j, init='k-means++', max_iter=750, n_init=10)\n",
        "        kmeans.fit(X_train[y_train == 0])\n",
        "        wcss.append(kmeans.inertia_)\n",
        "\n",
        "    # Finding the optimal number of clusters using the elbow method\n",
        "    optimal_num_clusters = np.argmin(np.diff(wcss)) + 2  # Adjust the index to account for starting from 2\n",
        "    print(f\"Run {i+1}, Optimal number of clusters: {optimal_num_clusters}\")\n",
        "\n",
        "    # Printing the number of majority and minority instances before undersampling\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before undersampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying cluster-ethereumd undersampling to the training data\n",
        "    X_train_res, y_train_res = cluster_ethereumd_undersampling(X_train, y_train, optimal_num_clusters)\n",
        "\n",
        "    # Printing the number of majority and minority instances after undersampling\n",
        "    print(\"After undersampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Creating a Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # Creating a GridSearchCV object\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    print(f\"Run {i+1} completed. Chosen C: {grid_search.best_params_['C']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_cbmu.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVK-q2wSEbhI"
      },
      "source": [
        "**Random oversampling + logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkpTgMM3IBBZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for logistic regression\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'max_iter': [3000],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_ros.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "    X_test_indices = X_test.index\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "\n",
        "    # Printing the number of majority and minority instances before applying Random Oversampling\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before Random Oversampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying Random Oversampling\n",
        "    ros = RandomOverSampler(sampling_strategy='not majority')\n",
        "    X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying Random Oversampling\n",
        "    print(\"After Random Oversampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\")\n",
        "\n",
        "    # Creating a Logistic Regression model\n",
        "    model = LogisticRegression()\n",
        "\n",
        "    # Creating a GridSearchCV object\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model obtained from GridSearchCV\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen \"C\" value for each run\n",
        "    print(f\"Run {i+1} completed. Chosen C: {best_params['C']}\")\n",
        "    print(f\"After ROS, Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"After ROS, Minority class samples (fraud): {sum(y_train_res == 1)}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_ros.csv.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdCpN7WfC1aW"
      },
      "source": [
        "**Smote + Logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF_ZDKyahqJF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for logistic regression and SMOTE\n",
        "param_grid = {\n",
        "    'smote__k_neighbors': [3, 5, 7],\n",
        "    'logistic__C': [0.1, 1, 10 ,100, 150],\n",
        "    'logistic__max_iter': [3000],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_smote.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('smote', SMOTE()),  # Ensuring 1:1 balance\n",
        "    ('logistic', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1', error_score='raise')\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying SMOTE\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before SMOTE:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying SMOTE to the training data\n",
        "    smote = SMOTE()\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying SMOTE\n",
        "    print(\"After SMOTE:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model obtained from GridSearchCV\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Chosen parameters: {best_params}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_smote.csv.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4jt7_8NE3aa"
      },
      "source": [
        "**ADASYN + Logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4_-hInW6RyP"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for logistic regression\n",
        "log_reg_param_grid = {\n",
        "    'logistic__C': [0.1, 1, 10 ,100, 150],\n",
        "    'logistic__max_iter': [2500],\n",
        "}\n",
        "\n",
        "# Defining the parameter grid for ADASYN\n",
        "adasyn_param_grid = {\n",
        "    'adasyn__n_neighbors': [3,5,7]\n",
        "}\n",
        "\n",
        "# Combining the parameter grids\n",
        "param_grid = {**log_reg_param_grid, **adasyn_param_grid}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_adasyn.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('adasyn', ADASYN(sampling_strategy=1.0)),  # Ensuring 1:1 balance\n",
        "    ('logistic', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying ADASYN\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before ADASYN:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying ADASYN\n",
        "    adasyn = ADASYN(sampling_strategy=1.0, n_neighbors=best_params['adasyn__n_neighbors'])\n",
        "    X_train_res, y_train_res = adasyn.fit_resample(X_train, y_train)\n",
        "    print(\"After ADASYN:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen \"C\" and \"n_neighbors\" values for each run\n",
        "    print(f\"Run {i+1} completed. Chosen C: {best_params['logistic__C']}, Chosen n_neighbors: {best_params['adasyn__n_neighbors']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_adasyn.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyFHu2-JGtxe"
      },
      "source": [
        "**Borderline smote 1 + Logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeIWkncP9wmr"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for logistic regression\n",
        "log_reg_param_grid = {\n",
        "    'logistic__C': [0.1, 1, 10 ,100, 150],\n",
        "    'logistic__max_iter': [3000],\n",
        "}\n",
        "\n",
        "# Defining the parameter grid for Borderline SMOTE2\n",
        "borderline_smote2_param_grid = {\n",
        "    'borderline_smote__k_neighbors': [3,5,7],\n",
        "    'borderline_smote__m_neighbors': [ 10, 15, 20]\n",
        "}\n",
        "\n",
        "# Combining the parameter grids\n",
        "param_grid = {**log_reg_param_grid, **borderline_smote2_param_grid}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_borderline_smote1.csv'\n",
        "\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('borderline_smote', BorderlineSMOTE(kind='borderline-1', sampling_strategy=1.0)),  # Ensuring 1:1 balance\n",
        "    ('logistic', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying Borderline SMOTE2\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before Borderline SMOTE2:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying Borderline SMOTE2\n",
        "    borderline_smote = BorderlineSMOTE(kind='borderline-2', sampling_strategy=1.0,\n",
        "                                       k_neighbors=best_params['borderline_smote__k_neighbors'],\n",
        "                                       m_neighbors=best_params['borderline_smote__m_neighbors'])\n",
        "    X_train_res, y_train_res = borderline_smote.fit_resample(X_train, y_train)\n",
        "    print(\"After Borderline SMOTE2:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen \"C\", \"k_neighbors\", and \"m_neighbors\" values for each run\n",
        "    print(f\"Run {i+1} completed. Chosen C: {best_params['logistic__C']}, Chosen k_neighbors: {best_params['borderline_smote__k_neighbors']}, Chosen m_neighbors: {best_params['borderline_smote__m_neighbors']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_borderline_smote1.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCiO-GxoHIGB"
      },
      "source": [
        "Borderline smote 2 + Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I2rdh0kiL5m"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for logistic regression\n",
        "log_reg_param_grid = {\n",
        "    'logistic__C': [0.01, 0.1, 1, 10, 100, 150],\n",
        "    'logistic__max_iter': [2500],\n",
        "}\n",
        "\n",
        "# Defining the parameter grid for Borderline SMOTE2\n",
        "borderline_smote2_param_grid = {\n",
        "    'borderline_smote__k_neighbors': [3, 5, 7],\n",
        "    'borderline_smote__m_neighbors': [10, 15, 20]\n",
        "}\n",
        "\n",
        "# Combining the parameter grids\n",
        "param_grid = {**log_reg_param_grid, **borderline_smote2_param_grid}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_borderline_smote2.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('borderline_smote', BorderlineSMOTE(kind='borderline-2', sampling_strategy=1.0)),  # Ensuring 1:1 balance\n",
        "    ('logistic', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying Borderline SMOTE2\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before Borderline SMOTE2:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying Borderline SMOTE2\n",
        "    borderline_smote = BorderlineSMOTE(kind='borderline-2', sampling_strategy=1.0,\n",
        "                                       k_neighbors=best_params['borderline_smote__k_neighbors'],\n",
        "                                       m_neighbors=best_params['borderline_smote__m_neighbors'])\n",
        "    X_train_res, y_train_res = borderline_smote.fit_resample(X_train, y_train)\n",
        "    print(\"After Borderline SMOTE2:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen \"C\", \"k_neighbors\", and \"m_neighbors\" values for each run\n",
        "    print(f\"Run {i+1} completed. Chosen C: {best_params['logistic__C']}, Chosen k_neighbors: {best_params['borderline_smote__k_neighbors']}, Chosen m_neighbors: {best_params['borderline_smote__m_neighbors']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_borderline_smote2.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neoYLwW-HZEO"
      },
      "source": [
        "**Conditional Generative Adversarial Network (cGAN) +logisitc regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz5KMDtnK2D7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization, Embedding, Flatten, multiply\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class cGAN():\n",
        "    def __init__(self, out_shape, latent_dim=32, learning_rate=0.0002, beta_1=0.5):\n",
        "        # Initializing latent dimension and output shape\n",
        "        self.latent_dim = latent_dim\n",
        "        self.out_shape = out_shape\n",
        "        self.num_classes = 2  # Binary classification (fraud vs non-fraud)\n",
        "\n",
        "        # Defining optimizer with given learning rate and beta_1\n",
        "        optimizer = Adam(learning_rate, beta_1)\n",
        "\n",
        "        # Building and compiling the discriminator model\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "        # Building the generator model\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Input for noise vector\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        # Input for class label\n",
        "        label = Input(shape=(1,))\n",
        "        # Generating samples using the generator\n",
        "        gen_samples = self.generator([noise, label])\n",
        "\n",
        "        # Freeze the discriminator weights when training the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Discriminator output for generated samples\n",
        "        valid = self.discriminator([gen_samples, label])\n",
        "\n",
        "        # Combined model (stacked generator and discriminator)\n",
        "        self.combined = Model([noise, label], valid)\n",
        "        self.combined.compile(loss=['binary_crossentropy'],\n",
        "                              optimizer=optimizer,\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "    def build_generator(self):\n",
        "        \"\"\"Builds the generator model.\"\"\"\n",
        "        model = Sequential()  # Initializing a sequential model\n",
        "        model.add(Dense(128, input_dim=self.latent_dim))  # Adding a dense layer with 128 units\n",
        "        model.add(LeakyReLU(alpha=0.2))  # Adding LeakyReLU activation\n",
        "        model.add(BatchNormalization(momentum=0.8))  # Add batch normalization\n",
        "        model.add(Dense(self.out_shape, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))  # Input for noise vector\n",
        "        label = Input(shape=(1,), dtype='int32')  # Input for class label\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))  # Embeding and flattening the label\n",
        "\n",
        "        model_input = multiply([noise, label_embedding])  # Multiplying noise and label embeddings\n",
        "        gen_sample = model(model_input)  # Generating sample\n",
        "\n",
        "        return Model([noise, label], gen_sample)  # Returning the generator model\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        \"\"\"Builds the discriminator model.\"\"\"\n",
        "        model = Sequential()  # Initializing a sequential model\n",
        "        model.add(Dense(128, input_dim=self.out_shape))  # Adding a dense layer with 128 units\n",
        "        model.add(LeakyReLU(alpha=0.2))  # Adding LeakyReLU activation\n",
        "        model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "        gen_sample = Input(shape=(self.out_shape,))  # Input for generated sample\n",
        "        label = Input(shape=(1,), dtype='int32')  # Input for class label\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))  # Embeding and flattening the label\n",
        "\n",
        "        model_input = multiply([gen_sample, label_embedding])  # Multiply generated sample and label embeddings\n",
        "        validity = model(model_input)  # Determining validity of the sample\n",
        "\n",
        "        return Model(inputs=[gen_sample, label], outputs=validity)  # Returning the discriminator model\n",
        "\n",
        "    def train(self, X_train, y_train, pos_index, neg_index, epochs=1500, batch_size=32, sample_interval=500):\n",
        "        # Createing arrays for real and fake labels\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Selecting a batch of positive and negative samples\n",
        "            idx1 = np.random.choice(pos_index, 8)\n",
        "            idx0 = np.random.choice(neg_index, batch_size-8)\n",
        "            idx = np.concatenate((idx1, idx0))\n",
        "            samples, labels = X_train[idx], y_train[idx]\n",
        "            samples, labels = shuffle(samples, labels, random_state=epoch)\n",
        "\n",
        "            # Generating a batch of noise vectors\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_samples = self.generator.predict([noise, labels], verbose=0)\n",
        "\n",
        "            # Applying label smoothing\n",
        "            if epoch < epochs // 1.5:\n",
        "                valid_smooth = (valid + 0.1) - (np.random.random(valid.shape) * 0.1)\n",
        "                fake_smooth = (fake - 0.1) + (np.random.random(fake.shape) * 0.1)\n",
        "            else:\n",
        "                valid_smooth = valid\n",
        "                fake_smooth = fake\n",
        "\n",
        "            # Training the discriminator\n",
        "            self.discriminator.trainable = True\n",
        "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Training the generator\n",
        "            self.discriminator.trainable = False\n",
        "            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n",
        "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
        "\n",
        "            # Printing the progress\n",
        "            if (epoch+1) % sample_interval == 0:\n",
        "                print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "        return self.generator\n",
        "\n",
        "# Identify binary columns\n",
        "binary_cols = [col for col in data.columns if data[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in data.columns if data[col].nunique() > 2 and col != 'isFraud']\n",
        "\n",
        "# Apply StandardScaler only to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "data[non_binary_cols] = scaler.fit_transform(data[non_binary_cols])\n",
        "\n",
        "X = data.drop(columns=['isFraud']).values\n",
        "y = data['isFraud'].values\n",
        "\n",
        "# Define latent dimensions to test\n",
        "latent_dims = [16, 32, 64]\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_lr_cgan.csv'\n",
        "\n",
        "\n",
        "# Running logistic regression with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    sample_size = min(100000, len(X))\n",
        "\n",
        "    # Performing stratified sampling to take a fixed number of samples\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled,)\n",
        "\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before cGAN:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Dictionary to store f1 scores for each latent_dim\n",
        "    f1_scores = {}\n",
        "\n",
        "    for latent_dim in latent_dims:\n",
        "        # Training the cGAN with the current latent_dim\n",
        "        generator = cGAN(out_shape=X_train.shape[1], latent_dim=latent_dim).train(\n",
        "            X_train, y_train, np.where(y_train == 1)[0], np.where(y_train == 0)[0])\n",
        "\n",
        "        # Generating synthetic samples to balance the training set\n",
        "        n_samples_to_generate = len(np.where(y_train == 0)[0]) - len(np.where(y_train == 1)[0])\n",
        "        noise = np.random.normal(0, 1, (n_samples_to_generate, latent_dim))\n",
        "        synthetic_samples = generator.predict([noise, np.ones((n_samples_to_generate, 1))], verbose=0)\n",
        "\n",
        "        X_train_balanced = np.vstack([X_train, synthetic_samples])\n",
        "        y_train_balanced = np.concatenate([y_train, np.ones(n_samples_to_generate)])\n",
        "\n",
        "        # Creating a Logistic Regression model with hyperparameter tuning\n",
        "        param_grid = {\n",
        "            'C': [1, 10, 100, 150],\n",
        "            'max_iter': [2500]\n",
        "        }\n",
        "\n",
        "        grid = GridSearchCV(LogisticRegression(), param_grid, scoring='f1', cv=3)\n",
        "        grid.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "        # Best model from grid search\n",
        "        best_model = grid.best_estimator_\n",
        "\n",
        "        # Predicting on the test set\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        # Calculating the f1 score\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        f1_scores[latent_dim] = f1\n",
        "\n",
        "        print(f\"Test f1 score with latent_dim {latent_dim}: {f1}\")\n",
        "\n",
        "    # Selecting the best latent_dim ethereumd on f1 score\n",
        "    best_latent_dim = max(f1_scores, key=f1_scores.get)\n",
        "    print(f\"Best latent_dim for Run {i+1}: {best_latent_dim}\")\n",
        "\n",
        "    # Training the cGAN with the best latent_dim\n",
        "    generator = cGAN(out_shape=X_train.shape[1], latent_dim=best_latent_dim).train(\n",
        "        X_train, y_train, np.where(y_train == 1)[0], np.where(y_train == 0)[0])\n",
        "\n",
        "    # Generating synthetic samples to balance the training set with the best latent_dim\n",
        "    n_samples_to_generate = len(np.where(y_train == 0)[0]) - len(np.where(y_train == 1)[0])\n",
        "    noise = np.random.normal(0, 1, (n_samples_to_generate, best_latent_dim))\n",
        "    synthetic_samples = generator.predict([noise, np.ones((n_samples_to_generate, 1))], verbose=0)\n",
        "\n",
        "    X_train_balanced = np.vstack([X_train, synthetic_samples])\n",
        "    y_train_balanced = np.concatenate([y_train, np.ones(n_samples_to_generate)])\n",
        "\n",
        "    print(f\"After cGAN:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_balanced == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_balanced == 1)}\")\n",
        "\n",
        "    # Creating a Logistic Regression model with hyperparameter tuning\n",
        "    grid = GridSearchCV(LogisticRegression(), param_grid, scoring='f1', cv=3)\n",
        "    grid.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "    # Best model from grid search\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    print(f\"Chosen hyperparameters for Run {i+1}: {grid.best_params_}\")\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': y_test.index,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0 and latent_dim == latent_dims[0]:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    print(f\"Run {i+1} completed. Best latent_dim: {best_latent_dim}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_lr_cgan.csv.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLq0eXtKYwM8"
      },
      "source": [
        "**Without balancing + Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeEEIO1acdy3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, 30 ,50],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_none.csv'\n",
        "\n",
        "\n",
        "# Running Random Forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Creating a Random Forest model\n",
        "    model = RandomForestClassifier()\n",
        "\n",
        "    # Creating a GridSearchCV object with the specified parameter grid\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Using the best model obtained from GridSearchCV\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Best parameters: {best_params}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_none.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dKL8TAF5XdO"
      },
      "source": [
        "**Random Undersampling + Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdurlfWX9QCM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "   'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, 30, 50],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_rus.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before undersampling\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before undersampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying random undersampling to the training data\n",
        "    rus = RandomUnderSampler(random_state=None)\n",
        "    X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after undersampling\n",
        "    print(\"After undersampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Creating a Random Forest model\n",
        "    model = RandomForestClassifier()\n",
        "\n",
        "    # Creating a GridSearchCV object\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Best parameters: {best_params}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_rus.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a8u6A0vaOjV"
      },
      "source": [
        "**Tomek links + Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpN0rNJfctrY"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, 30, 50],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_tomek.csv'\n",
        "\n",
        "\n",
        "# Running Random Forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying Tomek Links\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before Tomek Links:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying Tomek Links to the training data\n",
        "    tl = TomekLinks()\n",
        "    X_train_res, y_train_res = tl.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying Tomek Links\n",
        "    print(\"After Tomek Links:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Creating a Random Forest model\n",
        "    model = RandomForestClassifier()\n",
        "\n",
        "    # Creating a GridSearchCV object\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Best parameters: {best_params}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_tomek.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIGgcpXiaXEZ"
      },
      "source": [
        "**Cluster ethereumd majoirty undersampling + Random forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua0gyk7Lz7sH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [10, 20, 30, 50],\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_cbmu.csv'\n",
        "\n",
        "# Defining the cluster-ethereumd undersampling method\n",
        "def cluster_ethereumd_undersampling(X_train, y_train, n_clusters):\n",
        "\n",
        "    # Separating majority and minority classes\n",
        "    X_train_maj = X_train[y_train == 0]\n",
        "    X_train_min = X_train[y_train == 1]\n",
        "\n",
        "    # Applying KMeans clustering to the majority class\n",
        "    kmeans = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10)\n",
        "    kmeans.fit(X_train_maj)\n",
        "    cluster_labels = kmeans.labels_\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    # Calculating the number of samples to keep from each cluster\n",
        "    N_maj = len(X_train_maj)\n",
        "    N_min = len(X_train_min)\n",
        "    N_maj_i = np.bincount(cluster_labels)\n",
        "    r_i = N_maj_i / N_maj\n",
        "    s_i = np.round(N_min * r_i).astype(int)\n",
        "\n",
        "    # Selecting the nearest samples to the centroids\n",
        "    X_resampled_maj = []\n",
        "    for i in range(kmeans.n_clusters):\n",
        "        cluster_points = X_train_maj[cluster_labels == i]\n",
        "        centroid = centroids[i].reshape(1, -1)\n",
        "        distances = np.linalg.norm(cluster_points - centroid, axis=1)\n",
        "        sorted_indices = np.argsort(distances)\n",
        "        X_resampled_maj.append(pd.DataFrame(cluster_points).iloc[sorted_indices[:s_i[i]]])\n",
        "\n",
        "    X_resampled_maj = pd.concat(X_resampled_maj)\n",
        "    X_resampled = pd.concat([X_resampled_maj, X_train_min])\n",
        "    y_resampled = np.array([0] * len(X_resampled_maj) + [1] * len(X_train_min))\n",
        "\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# Running Random Forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Calculate the within-cluster sum of squares (WCSS) for different number of clusters\n",
        "    wcss = []\n",
        "    for j in range(2, 11):  # Start from 2 to avoid choosing 1 as the optimal number of clusters\n",
        "        kmeans = KMeans(n_clusters=j, init='k-means++', max_iter=750, n_init=10)\n",
        "        kmeans.fit(X_train[y_train == 0])\n",
        "        wcss.append(kmeans.inertia_)\n",
        "\n",
        "    # Finding the optimal number of clusters using the elbow method\n",
        "    optimal_num_clusters = np.argmin(np.diff(wcss)) + 2  # Adjust the index to account for starting from 2\n",
        "    print(f\"Run {i+1}, Optimal number of clusters: {optimal_num_clusters}\")\n",
        "\n",
        "    # Printing the number of majority and minority instances before undersampling\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before cluster-ethereumd undersampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying cluster-ethereumd undersampling to the training data using the optimal number of clusters\n",
        "    X_train_res, y_train_res = cluster_ethereumd_undersampling(X_train, y_train, optimal_num_clusters)\n",
        "\n",
        "    # Printing the number of majority and minority instances after undersampling\n",
        "    print(\"After cluster-ethereumd undersampling:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Creating a Random Forest model\n",
        "    model = RandomForestClassifier()\n",
        "\n",
        "    # Creating a GridSearchCV object\n",
        "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Chosen parameters: {best_params}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_cbmu.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBrv8-1KadW1"
      },
      "source": [
        "**Random Oversampling + Random forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU7Pv2JvlYLF"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from imblearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "# Defining the parameter grid for Random Forest\n",
        "param_grid = {\n",
        "    'randomforestclassifier__max_depth': [10, 20, 30, 50],\n",
        "    'randomforestclassifier__n_estimators': [100, 200, 500]\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_ros.csv'\n",
        "\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('ros', RandomOverSampler(sampling_strategy='not majority')),  # Ensuring equal balance\n",
        "    ('randomforestclassifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Running Random Forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying RandomOverSampler\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before RandomOverSampler:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying RandomOverSampler to the training data\n",
        "    ros = RandomOverSampler(sampling_strategy='not majority')\n",
        "    X_train_res, y_train_res = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying RandomOverSampler\n",
        "    print(\"After RandomOverSampler:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Chosen max_depth: {best_params['randomforestclassifier__max_depth']} and n_estimators: {best_params['randomforestclassifier__n_estimators']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_ros.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we4Z6PZkbC_h"
      },
      "source": [
        "**Smote + Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPIhfnj_lkgh"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from imblearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for Random Forest and SMOTE\n",
        "param_grid = {\n",
        "    'smote__k_neighbors': [3, 5, 7],\n",
        "    'randomforestclassifier__max_depth': [10, 20, 30, 50],\n",
        "    'randomforestclassifier__n_estimators': [100, 200, 500]\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_smote.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('smote', SMOTE()),  # Using SMOTE to handle class imbalance\n",
        "    ('randomforestclassifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Running Random Forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying SMOTE\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before SMOTE:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying SMOTE to the training data\n",
        "    smote = SMOTE()\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying SMOTE\n",
        "    print(\"After SMOTE:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Chosen k_neighbors: {best_params['smote__k_neighbors']}, max_depth: {best_params['randomforestclassifier__max_depth']}, n_estimators: {best_params['randomforestclassifier__n_estimators']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_smote.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J141rouDbY0H"
      },
      "source": [
        "**ADASYN + Random Forest**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qrJssjv2Tea"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "\n",
        "# Defining the parameter grid for random forest\n",
        "rf_param_grid = {\n",
        "    'randomforest__max_depth': [ 10, 20, 30, 50],\n",
        "    'randomforest__n_estimators': [100, 200, 500]\n",
        "}\n",
        "\n",
        "# Defining the parameter grid for ADASYN\n",
        "adasyn_param_grid = {\n",
        "    'adasyn__n_neighbors':  [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Combining the parameter grids\n",
        "param_grid = {**rf_param_grid, **adasyn_param_grid}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_adasyn.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('adasyn', ADASYN(sampling_strategy=1.0)),  # Ensuring 1:1 balance\n",
        "    ('randomforest', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Running random forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying ADASYN\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before ADASYN:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying ADASYN\n",
        "    adasyn = ADASYN(sampling_strategy=1.0, n_neighbors=best_params['adasyn__n_neighbors'])\n",
        "    X_train_res, y_train_res = adasyn.fit_resample(X_train, y_train)\n",
        "    print(\"After ADASYN:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\\n\")\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Chosen max_depth: {best_params['randomforest__max_depth']}, Chosen n_neighbors: {best_params['adasyn__n_neighbors']}, Chosen n_estimators: {best_params['randomforest__n_estimators']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_adasyn.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4ellU2fb5gZ"
      },
      "source": [
        "**Borderline smote 1 + random forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyxwjQgan91t"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from imblearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for Random Forest and Borderline SMOTE1\n",
        "param_grid = {\n",
        "    'borderlinesmote__k_neighbors': [3, 5, 7],\n",
        "    'borderlinesmote__m_neighbors': [10, 15, 20],\n",
        "    'randomforestclassifier__max_depth': [ 10, 20, 30,50],\n",
        "    'randomforestclassifier__n_estimators': [100, 200, 500]\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_borderlinesmote1.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('borderlinesmote', BorderlineSMOTE(kind='borderline-1', sampling_strategy='auto')),  # Using Borderline SMOTE1 to handle class imbalance\n",
        "    ('randomforestclassifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Running Random Forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying Borderline SMOTE1\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before Borderline SMOTE1:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying Borderline SMOTE1 to the training data\n",
        "    borderlinesmote = BorderlineSMOTE(kind='borderline-1', sampling_strategy='auto')\n",
        "    X_train_res, y_train_res = borderlinesmote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying Borderline SMOTE1\n",
        "    print(\"After Borderline SMOTE1:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Chosen k_neighbors: {best_params['borderlinesmote__k_neighbors']}, m_neighbors: {best_params['borderlinesmote__m_neighbors']}, max_depth: {best_params['randomforestclassifier__max_depth']}, n_estimators: {best_params['randomforestclassifier__n_estimators']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_borderlinesmote1.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbd5pnqPbizH"
      },
      "source": [
        "**Borderline smote2 + Random Forest**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfB5uzzt2DMF"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from imblearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud'])\n",
        "y = data['isFraud']\n",
        "\n",
        "# Identifying non-binary columns\n",
        "binary_cols = [col for col in X.columns if X[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in X.columns if X[col].nunique() > 2]\n",
        "\n",
        "# Applying StandardScaler to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "X[non_binary_cols] = scaler.fit_transform(X[non_binary_cols])\n",
        "\n",
        "# Defining the parameter grid for Random Forest and Borderline SMOTE1\n",
        "param_grid = {\n",
        "    'borderlinesmote__k_neighbors': [3, 5, 7],\n",
        "    'borderlinesmote__m_neighbors': [10, 15, 20],\n",
        "    'randomforestclassifier__max_depth': [10, 20, 30, 50 ],\n",
        "    'randomforestclassifier__n_estimators': [100, 200, 500]\n",
        "}\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_borderlinesmote2.csv'\n",
        "\n",
        "# Removing the output file if it already exists\n",
        "if os.path.exists(output_file):\n",
        "    os.remove(output_file)\n",
        "\n",
        "# Creating the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('borderlinesmote', BorderlineSMOTE(kind='borderline-2', sampling_strategy='auto')),  # Using Borderline SMOTE1 to handle class imbalance\n",
        "    ('randomforestclassifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "# Creating a GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='f1')\n",
        "\n",
        "# Running Random Forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    # Performing stratified sampling to take 50% of the data\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, test_size=0.5, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets using stratified sampling\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    # Printing the number of majority and minority instances before applying Borderline SMOTE1\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before Borderline SMOTE2:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Applying Borderline SMOTE1 to the training data\n",
        "    borderlinesmote = BorderlineSMOTE(kind='borderline-2', sampling_strategy='auto')\n",
        "    X_train_res, y_train_res = borderlinesmote.fit_resample(X_train, y_train)\n",
        "\n",
        "    # Printing the number of majority and minority instances after applying Borderline SMOTE1\n",
        "    print(\"After Borderline SMOTE2:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_res == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_res == 1)}\")\n",
        "\n",
        "    # Fitting the model to the training data\n",
        "    grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "    # Using the best model\n",
        "    best_model = grid_search.best_estimator_\n",
        "    best_params = grid_search.best_params_\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({\n",
        "        'Resample': [i + 1] * len(y_test),\n",
        "        'Original_Index': X_test_indices,\n",
        "        'Actual': y_test,\n",
        "        'Predicted': y_pred,\n",
        "        'Probability': y_prob\n",
        "    })\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    # Printing the chosen parameters for each run\n",
        "    print(f\"Run {i+1} completed. Chosen k_neighbors: {best_params['borderlinesmote__k_neighbors']}, m_neighbors: {best_params['borderlinesmote__m_neighbors']}, max_depth: {best_params['randomforestclassifier__max_depth']}, n_estimators: {best_params['randomforestclassifier__n_estimators']}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_borderlinesmote2.csv.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M_yahznb--7"
      },
      "source": [
        "**Conditional Generative Adversarial Network (cGAN) + Random forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKRPTs5eK2Gc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.layers import Input, Dense, LeakyReLU, BatchNormalization, Embedding, Flatten, multiply\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "\n",
        "class cGAN():\n",
        "    def __init__(self, out_shape, latent_dim=32, learning_rate=0.0002, beta_1=0.5):\n",
        "        # Initializing latent dimension and output shape\n",
        "        self.latent_dim = latent_dim\n",
        "        self.out_shape = out_shape\n",
        "        self.num_classes = 2  # Binary classification (fraud vs non-fraud)\n",
        "\n",
        "        # Defining optimizer with given learning rate and beta_1\n",
        "        optimizer = Adam(learning_rate, beta_1)\n",
        "\n",
        "        # Building and compiling the discriminator model\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "\n",
        "        # Building the generator model\n",
        "        self.generator = self.build_generator()\n",
        "\n",
        "        # Input for noise vector\n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        # Input for class label\n",
        "        label = Input(shape=(1,))\n",
        "        # Generating samples using the generator\n",
        "        gen_samples = self.generator([noise, label])\n",
        "\n",
        "        # Freeze the discriminator weights when training the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Discriminator output for generated samples\n",
        "        valid = self.discriminator([gen_samples, label])\n",
        "\n",
        "        # Combined model (stacked generator and discriminator)\n",
        "        self.combined = Model([noise, label], valid)\n",
        "        self.combined.compile(loss=['binary_crossentropy'],\n",
        "                              optimizer=optimizer,\n",
        "                              metrics=['accuracy'])\n",
        "\n",
        "    def build_generator(self):\n",
        "        \"\"\"Builds the generator model.\"\"\"\n",
        "        model = Sequential()  # Initializing a sequential model\n",
        "        model.add(Dense(128, input_dim=self.latent_dim))  # Adding a dense layer with 128 units\n",
        "        model.add(LeakyReLU(alpha=0.2))  # Adding LeakyReLU activation\n",
        "        model.add(BatchNormalization(momentum=0.8))  # Add batch normalization\n",
        "        model.add(Dense(self.out_shape, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "        noise = Input(shape=(self.latent_dim,))  # Input for noise vector\n",
        "        label = Input(shape=(1,), dtype='int32')  # Input for class label\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))  # Embed and flatten the label\n",
        "\n",
        "        model_input = multiply([noise, label_embedding])  # Multiply noise and label embeddings\n",
        "        gen_sample = model(model_input)  # Generating sample\n",
        "\n",
        "        return Model([noise, label], gen_sample)  # Returning the generator model\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        \"\"\"Builds the discriminator model.\"\"\"\n",
        "        model = Sequential()  # Initializing a sequential model\n",
        "        model.add(Dense(128, input_dim=self.out_shape))  # Adding a dense layer with 128 units\n",
        "        model.add(LeakyReLU(alpha=0.2))  # Adding LeakyReLU activation\n",
        "        model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation\n",
        "\n",
        "        gen_sample = Input(shape=(self.out_shape,))  # Input for generated sample\n",
        "        label = Input(shape=(1,), dtype='int32')  # Input for class label\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))  # Embeding and flattening the label\n",
        "\n",
        "        model_input = multiply([gen_sample, label_embedding])  # Multiply generated sample and label embeddings\n",
        "        validity = model(model_input)  # Determining validity of the sample\n",
        "\n",
        "        return Model(inputs=[gen_sample, label], outputs=validity)  # Returning the discriminator model\n",
        "\n",
        "    def train(self, X_train, y_train, pos_index, neg_index, epochs=1500, batch_size=32, sample_interval=500):\n",
        "        # Createing arrays for real and fake labels\n",
        "        valid = np.ones((batch_size, 1))\n",
        "        fake = np.zeros((batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Selecting a batch of positive and negative samples\n",
        "            idx1 = np.random.choice(pos_index, 8)\n",
        "            idx0 = np.random.choice(neg_index, batch_size-8)\n",
        "            idx = np.concatenate((idx1, idx0))\n",
        "            samples, labels = X_train[idx], y_train[idx]\n",
        "            samples, labels = shuffle(samples, labels, random_state=epoch)\n",
        "\n",
        "            # Generating a batch of noise vectors\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "            gen_samples = self.generator.predict([noise, labels], verbose=0)\n",
        "\n",
        "            # Applying label smoothing\n",
        "            if epoch < epochs // 1.5:\n",
        "                valid_smooth = (valid + 0.1) - (np.random.random(valid.shape) * 0.1)\n",
        "                fake_smooth = (fake - 0.1) + (np.random.random(fake.shape) * 0.1)\n",
        "            else:\n",
        "                valid_smooth = valid\n",
        "                fake_smooth = fake\n",
        "\n",
        "            # Training the discriminator\n",
        "            self.discriminator.trainable = True\n",
        "            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n",
        "            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # Training the generator\n",
        "            self.discriminator.trainable = False\n",
        "            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n",
        "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
        "\n",
        "            # Printing the progress\n",
        "            if (epoch+1) % sample_interval == 0:\n",
        "                print(f\"{epoch} [D loss: {d_loss[0]}, acc.: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
        "\n",
        "        return self.generator\n",
        "\n",
        "# Identify binary columns\n",
        "binary_cols = [col for col in data.columns if data[col].nunique() == 2]\n",
        "non_binary_cols = [col for col in data.columns if data[col].nunique() > 2 and col != 'isFraud']\n",
        "\n",
        "# Apply StandardScaler only to non-binary columns\n",
        "scaler = StandardScaler()\n",
        "data[non_binary_cols] = scaler.fit_transform(data[non_binary_cols])\n",
        "\n",
        "X = data.drop(columns=['isFraud']).values\n",
        "y = data['isFraud'].values\n",
        "\n",
        "# Define latent dimensions to test\n",
        "latent_dims = [16, 32, 64]\n",
        "\n",
        "# File path for saving results\n",
        "output_file = 'ethereum_rf_cgan.csv'\n",
        "\n",
        "\n",
        "# Running Random Forest with hyperparameter tuning twelve times\n",
        "for i in range(12):\n",
        "    sample_size = min(100000, len(X))\n",
        "\n",
        "    # Performing stratified sampling to take a fixed number of samples\n",
        "    X_sampled, _, y_sampled, _ = train_test_split(X, y, train_size=sample_size, stratify=y)\n",
        "\n",
        "    # Splitting the sampled dataset into training and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=0.2, stratify=y_sampled)\n",
        "    X_test_indices = X_test.index\n",
        "    print(f\"Run {i+1}:\")\n",
        "    print(\"Before cGAN:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train == 1)}\")\n",
        "\n",
        "    # Dictionary to store f1 scores for each latent_dim\n",
        "    f1_scores = {}\n",
        "\n",
        "    for latent_dim in latent_dims:\n",
        "        # Training the cGAN with the current latent_dim\n",
        "        generator = cGAN(out_shape=X_train.shape[1], latent_dim=latent_dim).train(\n",
        "            X_train, y_train, np.where(y_train == 1)[0], np.where(y_train == 0)[0])\n",
        "\n",
        "        # Generating synthetic samples to balance the training set\n",
        "        n_samples_to_generate = len(np.where(y_train == 0)[0]) - len(np.where(y_train == 1)[0])\n",
        "        noise = np.random.normal(0, 1, (n_samples_to_generate, latent_dim))\n",
        "        synthetic_samples = generator.predict([noise, np.ones((n_samples_to_generate, 1))], verbose=0)\n",
        "\n",
        "        X_train_balanced = np.vstack([X_train, synthetic_samples])\n",
        "        y_train_balanced = np.concatenate([y_train, np.ones(n_samples_to_generate)])\n",
        "\n",
        "        # Creating a Random Forest model with hyperparameter tuning\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 500],\n",
        "            'max_depth': [10, 20, 30,50],\n",
        "        }\n",
        "\n",
        "        grid = GridSearchCV(RandomForestClassifier(), param_grid, scoring='f1', cv=3)\n",
        "        grid.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "        # Best model from grid search\n",
        "        best_model = grid.best_estimator_\n",
        "\n",
        "        # Predicting on the test set\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        # Calculating the f1 score\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        f1_scores[latent_dim] = f1\n",
        "\n",
        "        print(f\"Test f1 score with latent_dim {latent_dim}: {f1}\")\n",
        "\n",
        "    # Selecting the best latent_dim ethereumd on f1 score\n",
        "    best_latent_dim = max(f1_scores, key=f1_scores.get)\n",
        "    print(f\"Best latent_dim for Run {i+1}: {best_latent_dim}\")\n",
        "\n",
        "    # Training the cGAN with the best latent_dim\n",
        "    generator = cGAN(out_shape=X_train.shape[1], latent_dim=best_latent_dim).train(\n",
        "        X_train, y_train, np.where(y_train == 1)[0], np.where(y_train == 0)[0])\n",
        "\n",
        "    # Generating synthetic samples to balance the training set with the best latent_dim\n",
        "    n_samples_to_generate = len(np.where(y_train == 0)[0]) - len(np.where(y_train == 1)[0])\n",
        "    noise = np.random.normal(0, 1, (n_samples_to_generate, best_latent_dim))\n",
        "    synthetic_samples = generator.predict([noise, np.ones((n_samples_to_generate, 1))], verbose=0)\n",
        "\n",
        "    X_train_balanced = np.vstack([X_train, synthetic_samples])\n",
        "    y_train_balanced = np.concatenate([y_train, np.ones(n_samples_to_generate)])\n",
        "\n",
        "    print(f\"After cGAN:\")\n",
        "    print(f\"Majority class samples (non-fraud): {sum(y_train_balanced == 0)}\")\n",
        "    print(f\"Minority class samples (fraud): {sum(y_train_balanced == 1)}\")\n",
        "\n",
        "    # Creating a Random Forest model with hyperparameter tuning\n",
        "    grid = GridSearchCV(RandomForestClassifier(), param_grid, scoring='f1', cv=3)\n",
        "    grid.fit(X_train_balanced, y_train_balanced)\n",
        "\n",
        "    # Best model from grid search\n",
        "    best_model = grid.best_estimator_\n",
        "\n",
        "    print(f\"Chosen hyperparameters for Run {i+1}: {grid.best_params_}\")\n",
        "\n",
        "    # Predicting on the test set\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_prob = best_model.predict_proba(X_test)[:, 1]  # Getting the probability of the positive class\n",
        "\n",
        "    # Creating a DataFrame with actual, predicted labels, and probabilities\n",
        "    results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred, 'Probability': y_prob})\n",
        "\n",
        "    # Appending results to the output file\n",
        "    if i == 0 and latent_dim == latent_dims[0]:\n",
        "        results.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        results.to_csv(output_file, index=False, mode='a', header=False)\n",
        "\n",
        "    print(f\"Run {i+1} completed. Best latent_dim: {best_latent_dim}\")\n",
        "\n",
        "print(\"All runs completed. Results saved to ethereum_rf_cgan.csv.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiWpIIXOxfno",
        "outputId": "3768d671-1e15-4fbe-c621-af2c547d6bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average proportion of minority samples in the k-nearest neighbors: 0.3434959349593496\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud']).values\n",
        "y = data['isFraud'].values\n",
        "\n",
        "# Find nearest neighbors in the majority class for each minority sample\n",
        "minority_indices = np.where(y == 1)[0]\n",
        "majority_indices = np.where(y == 0)[0]\n",
        "\n",
        "k = 5  # Number of neighbors\n",
        "nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
        "_, indices = nbrs.kneighbors(X[minority_indices])\n",
        "\n",
        "# Calculate proportion of minority samples in the k-nearest neighbors\n",
        "minority_proportion = np.mean([np.sum(y[neighbors] == 1) / k for neighbors in indices])\n",
        "print(f\"Average proportion of minority samples in the k-nearest neighbors: {minority_proportion}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "pCGSgxh2SYfv",
        "outputId": "1eb324fe-3411-4e90-b591-e2b2ccfffa8d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of minority samples with 0 majority neighbors: 48\n",
            "Number of minority samples with 1 majority neighbors: 12\n",
            "Number of minority samples with 2 majority neighbors: 26\n",
            "Number of minority samples with 3 majority neighbors: 73\n",
            "Number of minority samples with 4 majority neighbors: 333\n",
            "Number of minority samples with 5 majority neighbors: 0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Defining features and target variable\n",
        "X = data.drop(columns=['isFraud']).values\n",
        "y = data['isFraud'].values\n",
        "\n",
        "# Identify minority and majority indices\n",
        "minority_indices = np.where(y == 1)[0]\n",
        "majority_indices = np.where(y == 0)[0]\n",
        "\n",
        "# Check k-nearest neighbors\n",
        "k = 5  # Number of neighbors\n",
        "nbrs = NearestNeighbors(n_neighbors=k).fit(X)\n",
        "_, indices = nbrs.kneighbors(X[minority_indices])\n",
        "\n",
        "# Calculate the count of majority samples in the k-nearest neighbors for each minority sample\n",
        "majority_counts = [np.sum(y[neighbors] == 0) for neighbors in indices]\n",
        "\n",
        "# Plot the histogram of majority counts\n",
        "plt.hist(majority_counts, bins=range(k + 2), edgecolor='k', alpha=0.7, align='left')\n",
        "plt.title('Histogram of Majority Class Neighbors Count in k-Nearest Neighbors of Minority Samples')\n",
        "plt.xlabel('Number of Majority Neighbors')\n",
        "plt.ylabel('Number of Minority Samples')\n",
        "plt.xticks(range(k + 1))\n",
        "plt.show()\n",
        "\n",
        "# Print out the count of minority samples with each possible number of majority neighbors\n",
        "for i in range(k + 1):\n",
        "    count = np.sum(np.array(majority_counts) == i)\n",
        "    print(f\"Number of minority samples with {i} majority neighbors: {count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEuNNxgXlpj-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
